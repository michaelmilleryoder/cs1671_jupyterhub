{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3dc1cc6-10e3-43e0-9b2a-9c3ab64ca08d",
   "metadata": {},
   "source": [
    "# Tf-idf document representations of headlines\n",
    "Back to clickbait! Let's take a look at how tf-idf document representations compare to raw counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc00fe5-945f-4d1a-8089-8fbd3e945426",
   "metadata": {},
   "source": [
    "## Load clickbait data from Kaggle\n",
    "This data consists of headlines classified as clickbait or not (regular news). It is from a dataset on Kaggle, a site where machine learning competitions and datasets are often hosted. Source site: https://www.kaggle.com/datasets/amananandrai/clickbait-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90300de0-f52f-4122-a9bf-551271e1ff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the dataset with pandas\n",
    "# 0 corresponds to not clickbait, 1 has been judged as clickbait\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Set pandas to display entire texts in dataframes\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "data = pd.read_csv('data/clickbait_data.csv')\n",
    "data.info()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e8cc3e-5f01-448c-ad24-4522945d4f8a",
   "metadata": {},
   "source": [
    "## Split into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0adbeff-1ebf-47b4-a88a-3711794e9681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_size = int(0.1 * len(data))\n",
    "train, test  = train_test_split(data, test_size=test_size, random_state=9)\n",
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d809d550-dfb8-4182-b288-ae65e56de1d6",
   "metadata": {},
   "source": [
    "## Extract unigram (raw bag-of-word count) features from the text data\n",
    "\"Features\" are data fields or attributes \"extracted\" from raw data, in our case, text data. The features were are examining here are \"unigram\" features, unique sequences of 1 word. This step converts each headline to a numeric vector of unigram counts (how many times each word type occurs).\n",
    "\"Training\" the vectorizer means finding how many unique features (in this case, unique words) are in the training set. This sets the number of columns in the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f76f8e-26ca-48b4-9a76-21f728f99a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "\n",
    "unigram_vectorizer = CountVectorizer(tokenizer=nltk.word_tokenize)\n",
    "unigram_vectorizer.fit(train['headline']) # input is a list of strings (documents)\n",
    "train_features = unigram_vectorizer.transform(train['headline'])\n",
    "test_features = unigram_vectorizer.transform(test['headline'])\n",
    "\n",
    "print(type(train_features))\n",
    "print(train_features.shape) # prints (number of rows in the matrix, number of columns)\n",
    "print(test_features.shape)  # prints (number of rows in the matrix, number of columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30f79a2-8b03-4557-90cc-b6a19e304fc8",
   "metadata": {},
   "source": [
    "Let's explore those training set unigram features a bit more. First convert the `scipy` sparse matrix into a regular `numpy` matrix to take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab18ef8-1ce6-47d8-a1ac-2c8b19b16e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_features = train_features.A\n",
    "print(type(unigram_features))\n",
    "print(unigram_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc38c76-77ef-4ee3-8e50-eb89176773b1",
   "metadata": {},
   "source": [
    "Let's take a look at a few example headline vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171de09f-40b8-4408-92bf-7d9967fe45fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run this cell with as many different sample indexes as you like\n",
    "\n",
    "sample_index = # FILL IN a random number less than the number of rows (datapoints) in ngram_features here. \n",
    "train.iloc[sample_index] # Take a look at the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ef672b-f8f6-4841-89d1-ce2389a28255",
   "metadata": {},
   "source": [
    "Label the nonzero features with the words they correspond to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2295d6a-154b-452f-8334-8684e5e96a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a pandas dataframe from the ngram features and label the column with their corresponding feature (unigram or word type)\n",
    "feature_names = unigram_vectorizer.get_feature_names_out()\n",
    "print(len(feature_names))\n",
    "\n",
    "unigram_feature_matrix = pd.DataFrame(unigram_features, columns=feature_names)\n",
    "\n",
    "# View the nonzero values in the feature vector for the example headline\n",
    "column_mask = unigram_feature_matrix.loc[sample_index].apply(lambda x: x > 0)\n",
    "nonzero_columns = column_mask[column_mask == True]\n",
    "unigram_feature_matrix.loc[[sample_index], nonzero_columns.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2acd4d-68a5-46b3-a094-85793cfe737d",
   "metadata": {},
   "source": [
    "## Extract **tf-idf-weighted** unigram features from the text data\n",
    "Let's take a look at how the document vectors change when we use tf-idf instead of raw counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66118989-e8f8-4c8a-8787-a20554bf2196",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=nltk.word_tokenize)\n",
    "tfidf_vectorizer.fit(train['headline']) # input is a list of strings (documents)\n",
    "train_features = tfidf_vectorizer.transform(train['headline'])\n",
    "test_features = tfidf_vectorizer.transform(test['headline'])\n",
    "\n",
    "print(type(train_features))\n",
    "print(train_features.shape) # prints (number of rows in the matrix, number of columns)\n",
    "print(test_features.shape)  # prints (number of rows in the matrix, number of columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914809af-ebfd-427a-9df5-2a3eb5e779a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the feature matrix to a NumPy array for inspection\n",
    "tfidf_features = train_features.A\n",
    "print(tfidf_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce4b0be-260c-47c4-9254-482384c0e39c",
   "metadata": {},
   "source": [
    "Let's take a look at an example headline vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c45281-ac38-497a-a621-c2f05bda791b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a pandas dataframe from the ngram features and label the column with their corresponding feature (unigram or word type)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_feature_matrix = pd.DataFrame(tfidf_features, columns=feature_names)\n",
    "\n",
    "# View the nonzero values in the feature vector for the example headline\n",
    "column_mask = tfidf_feature_matrix.loc[sample_index].apply(lambda x: x > 0)\n",
    "nonzero_columns = column_mask[column_mask == True]\n",
    "tfidf_feature_matrix.loc[[sample_index], nonzero_columns.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00f595d-51d8-422a-b338-13582bc9adf7",
   "metadata": {},
   "source": [
    "**How do the weighted counts differ for common words and rarer words?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90d89c9-1e96-4064-a955-620590025d27",
   "metadata": {},
   "source": [
    "# Find similar documents with unweighted unigram count vectors\n",
    "We can use the numeric feature vectors computed for every headline to compute similarities with other headlines using cosine similarity. Though contemporary information retreival (search engine) systems are of course much more complex, they still use this basic framework to return results: convert texts to vectors and return the most similar documents to your query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5fb870-1980-41d7-a6a8-4495cba1a19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity between the sample headline vector and all other headlines in the training set\n",
    "\n",
    "from scipy.spatial.distance import cosine # cosine distance from the scipy package\n",
    "\n",
    "def compute_cosine_similarity_to_sample(vector):\n",
    "    \"\"\" Compute cosine similarity with sample vector \"\"\"\n",
    "    return 1 - cosine(vector, sample_vector)\n",
    "\n",
    "sample_vector = unigram_features[sample_index]\n",
    "sample_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edd10bb-aad7-436c-b7f4-d5b8c71a76b4",
   "metadata": {},
   "source": [
    "Now let's rank similarities and find out which vectors are most similar to the sample headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063ed19d-afad-4b8c-84cb-30c6293cad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity from every other document's vector to the sample document\n",
    "similarities = unigram_feature_matrix.apply(compute_cosine_similarity_to_sample, axis=1) # apply function over every row in the df\n",
    "sorted_similarities = similarities.sort_values(ascending=False)\n",
    "\n",
    "# Create a pandas dataframe of all other headlines, sorted by cosine similarity with the sample\n",
    "similar_headlines = train.iloc[sorted_similarities.index].copy().reset_index(drop=True)\n",
    "similar_headlines['cosine_similarity'] = sorted_similarities.values\n",
    "similar_headlines.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e94d7e4-8e0f-437a-8078-1a9dcaa7bbea",
   "metadata": {},
   "source": [
    "# Find similar documents with **tf-idf-weighted** unigram count vectors\n",
    "We can use the numeric feature vectors computed for every headline to compute similarities with other headlines using cosine similarity. Though contemporary information retrieval (search engine) systems are of course much more complex, they still use this basic framework to return results: convert texts to vectors and return the most similar documents to your query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d0381d-24c5-4736-a710-77024b236068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity between the sample headline vector and all other headlines in the training set\n",
    "sample_vector = tfidf_features[sample_index]\n",
    "sample_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8573fa-1386-43e5-afe6-7e2da4d4f9d4",
   "metadata": {},
   "source": [
    "Now let's rank similarities and find out which vectors are most similar to the sample headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80652506-f2ea-4792-8202-358e147497f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity from every other document's vector to the sample document\n",
    "similarities = tfidf_feature_matrix.apply(compute_cosine_similarity_to_sample, axis=1) # apply function over every row in the df\n",
    "sorted_similarities = similarities.sort_values(ascending=False)\n",
    "\n",
    "# Create a pandas dataframe of all other headlines, sorted by cosine similarity with the sample\n",
    "similar_headlines = train.iloc[sorted_similarities.index].copy().reset_index(drop=True)\n",
    "similar_headlines['cosine_similarity'] = sorted_similarities.values\n",
    "similar_headlines.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917441b3-f19d-4af5-ba9a-d10b45bf63df",
   "metadata": {},
   "source": [
    "**How do these results compare with the most similar documents from raw counts?**  \n",
    "Do they seem better or worse in any particular way? Do you see the influence of the goal of tf-idf weighting, i.e. downweighting common words?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d03e681-2b27-456e-b6e6-d4478944b063",
   "metadata": {},
   "source": [
    "# Evaluate clickbait classifiers\n",
    "Let's switch gears to building a classifier for this clickbait dataset, as we did before with Naive Bayes. No need to worry about what Naive Bayes is, it's sufficient to know that it's a simple machine learning classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b489089f-c4f7-4f1d-85d7-6055c44d16f6",
   "metadata": {},
   "source": [
    "## Evaluation on a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b82b845-2adc-4606-8736-b68f3f60c3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB() # Instantiate a Naive Bayes classifier\n",
    "clf.fit(train_features, train['clickbait']) # Train the Naive Bayes classifier on features previously extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5addd1-7083-4496-89fd-7f8ea3ba18b4",
   "metadata": {},
   "source": [
    "Way back in session 4 we made individual predictions with our classifier. Now let's evaluate the classifier on a test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dec200-f35c-42a5-bfff-c04591f35b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report # this automatically provides a bunch of useful evaluation metrics\n",
    "\n",
    "test_labels = test['clickbait'] # true (gold) test set labels for clickbait/not clickbait\n",
    "test_predictions = clf.predict(test_features)\n",
    "\n",
    "results = pd.DataFrame(classification_report(test_labels, test_predictions, output_dict=True))\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c8ff5a-4aba-4c5c-aded-ac34b72b658b",
   "metadata": {},
   "source": [
    "Try to interpret these results. `0` and `1` refer to the two different classes (output values): clickbait (1) and non-clickbait (0). **Why are precision and recall different for different classes?**  \n",
    "`support` refers to how many datapoints (rows) are classified in each of the 2 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e880569-4a16-4ab7-a0c3-f472c7108f13",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "Let's evaluate using cross validation now. To do so, we'll make a scikit-learn `pipeline`, which allows us to combine our preprocessing and training steps. This is important so that we extract features from **only the training folds**, not from everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e490895-aa91-4caa-91fc-57d4fefc82e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "clf = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "X_input = # FILL IN column of raw text data as input to our pipeline\n",
    "X_labels = # FILL IN column of labels for that data\n",
    "num_folds = # FILL IN number of folds to try\n",
    "\n",
    "scores = cross_validate(clf, X_input, X_labels, scoring=['accuracy', 'precision', 'recall', 'f1'], cv=num_folds, return_train_score=False)\n",
    "scores = pd.DataFrame(scores)[['test_accuracy', 'test_precision', 'test_recall', 'test_f1']]\n",
    "scores.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
