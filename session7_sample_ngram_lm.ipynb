{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3dc1cc6-10e3-43e0-9b2a-9c3ab64ca08d",
   "metadata": {},
   "source": [
    "# N-gram language modeling\n",
    "Let's train n-gram language models that better handle 0 probabilities and then sample (generate) from them!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd1bb16-d13a-488c-800b-5f8fb4f58f27",
   "metadata": {},
   "source": [
    "## Setup\n",
    "No need to run this unless you haven't successfully installed `nltk` or `scikit-learn` yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6443dda-9967-4c06-8dc9-ef35e1c3c2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --user nltk\n",
    "! pip install --user scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5736dec-2e61-4429-85a6-4c1942cd55dc",
   "metadata": {},
   "source": [
    "Now select **Kernel > Restart Kernel** from the menu bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775107bc-b1f7-46b0-a5bc-43191c6715d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test importing\n",
    "import nltk\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08935b6c-e29b-419a-bf52-383ef521024c",
   "metadata": {},
   "source": [
    "# Train an n-gram language model on Reuters data that handles 0 probabilities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bcc00fe5-945f-4d1a-8089-8fbd3e945426",
   "metadata": {},
   "source": [
    "## Load news text from Reuters\n",
    "Reuters from the '90s. Old news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7884d4-c096-47cc-acca-3f9c3e621fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only need to run this once on your CRCD account\n",
    "import nltk\n",
    "nltk.download('reuters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20c0dff-e1eb-402e-a85f-e9de437bc5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218266c3-17e3-466b-bf4d-9c1b20113537",
   "metadata": {},
   "source": [
    "## Preprocess the text into ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e527af31-225a-49c5-9a08-5a668dc496ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = reuters.sents() # Load all sentences in the corpus and lowercase\n",
    "\n",
    "# Lowercase the data\n",
    "sents = [[word.lower() for word in doc] for doc in sents]\n",
    "sents[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fe676a-045e-4a01-870a-dbb38ae08343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly split into training and test sets to evaluate perplexity\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_seed = # FILL IN an integer\n",
    "train, test = train_test_split(sents, test_size=0.1, random_state=random_seed)\n",
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3b4b64-d922-40ac-8eac-cfadf0998757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the input for n-gram language model training with NLTK (sequences of n-grams)\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "n = # what order of ngram (2 for bigram, 3 for trigram, etc)\n",
    "processed_train, vocab = padded_everygram_pipeline(n, train) \n",
    "# unfortunately can't inspect this as it's a generator that's evaluated (\"filled in\") lazily"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45982437-4148-4348-8cb6-2dfb79a65b7d",
   "metadata": {},
   "source": [
    "## Train an n-gram language model with Lidstone smoothing\n",
    "To handle `<UNK>` (unseen) words, we will add a small pseudocount of 0.001, which is called \"Lidstone\" smoothing as the pseudocount $\\gamma$ is $0 < \\gamma < 1$. At $\\gamma = 1$, this is the same as Laplace smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea2a07a-6051-44d7-bbdf-de0ca16de8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import Lidstone\n",
    "\n",
    "# Initiate and fit an ngram language model\n",
    "lm = Lidstone(gamma=0.001, order=n) # you can also play around with changing gamma\n",
    "lm.fit(processed_train, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c6a5e6-25b2-4cef-8e86-322d8fd1870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test how it handles unseen words\n",
    "weird_word = 'weirdo' # FILL IN a rare word that likely does not occur in '90s news\n",
    "print(lm.vocab.lookup(weird_word)) # How is it treating that word?\n",
    "print(lm.score(weird_word)) # What probability is it giving that word in the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b944d7-95ea-4874-b52c-630fa2ff572d",
   "metadata": {},
   "source": [
    "## Evaluate perplexity on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763d4ce5-276c-4f44-8922-b6bd8ba637ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test set into the same input format as the training set\n",
    "\n",
    "processed_test_generator, vocab = padded_everygram_pipeline(n, test) \n",
    "processed_test = [list(el) for el in list(processed_test_generator)]\n",
    "len(processed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aa8117-0c3e-4278-9dd3-b19d61537094",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.perplexity(processed_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea35609b-e1db-4f53-b3d2-0fd9884af654",
   "metadata": {},
   "source": [
    "Look at that! A real value instead of `inf`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7435f2-88ea-4dcb-9bba-b5a49b000fd1",
   "metadata": {},
   "source": [
    "## Sample (generate) from this trained language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd2bc9c-6b18-49dd-b41d-dc099e695b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell as many times as you like to take new samples (generate new phrases)\n",
    "# Record or copy the cell if there are any good ones you want to save and report back to the clas\n",
    "\n",
    "num_tokens =  # FILL IN with how many tokens you want to generate\n",
    "prompt = [] # FILL IN with a list of tokens as a prompt (prior context). Or pass an empty list to just start generating\n",
    "generated_toks = lm.generate(num_words=num_tokens, text_seed=prompt)\n",
    "' '.join(prompt + generated_toks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91322794-9478-4d29-a574-b35d6beb56dd",
   "metadata": {},
   "source": [
    "# Train an n-gram language model from a different data source\n",
    "Alright, let's train an n-gram language model from different data sources. You can choose from the following options, or try loading some other text data if you want!\n",
    "* Airbnb descriptions\n",
    "* Shakespeare plays\n",
    "\n",
    "Whichever you choose, you can skip to the corresponding part of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56d9f93-64f3-4bd9-9b46-e0bd41450f33",
   "metadata": {},
   "source": [
    "## Airbnb descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a7a051-bc55-44e5-bdfc-7d381b50d9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "import pandas as pd\n",
    "\n",
    "airbnb_filepath = '' # FILL IN the filepath to the CSV file with the Airbnb listings you should have somewhere still from session 2\n",
    "# If you don't have any Airbnb data, open and run session2_text_normalization.ipynb\n",
    "listings = pd.read_csv(airbnb_filepath) # reads CSV file into a pandas dataframe\n",
    "len(listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935f9275-de43-44ef-bd95-36f5d4ecdc16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preprocess description column\n",
    "from nltk import word_tokenize\n",
    "from tqdm.auto import tqdm # for progress bar\n",
    "tqdm.pandas()\n",
    "\n",
    "def preprocess_airbnb(text):\n",
    "    stray_html = ' ' # FILL IN with HTML tag that we removed from this data earlier. Or if you forget this, just leave it as a blank\n",
    "    processed = text.replace(stray_html, ' ').lower()\n",
    "    return word_tokenize(processed)\n",
    "\n",
    "processed_airbnb = listings.description.dropna().progress_map(preprocess_airbnb).tolist()\n",
    "processed_airbnb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e797be-e735-4a18-9e34-d96d1758de12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input for NLTK\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "n = # what order of ngram\n",
    "listings_input, vocab = padded_everygram_pipeline(n, processed_airbnb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ecb539-d865-4e83-8a2c-3a919c7b9712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train n-gram language model\n",
    "from nltk.lm import Lidstone\n",
    "\n",
    "# Initiate and fit an ngram language model\n",
    "lm = Lidstone(gamma=0.001, order=n) # You can also play around with change the gamma value\n",
    "lm.fit(listings_input, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb683539-7fa3-40f1-bbe2-ce355cc5a945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell as many times as you like to take new samples (generate new phrases)\n",
    "# Record or copy the cell if there are any good ones you want to save and report back to the clas\n",
    "\n",
    "num_tokens =  # FILL IN with how many tokens you want to generate\n",
    "prompt = [] # FILL IN with a list of tokens as a prompt (prior context). Or pass an empty list to just start generating\n",
    "generated_toks = lm.generate(num_words=num_tokens, text_seed=prompt)\n",
    "' '.join(prompt + generated_toks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190dd55a-0fb9-489c-9b15-3337edf59c70",
   "metadata": {},
   "source": [
    "## Shakespeare plays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d31dab-c62c-4f63-b083-bf7c9cb14507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Shakespeare plays\n",
    "\n",
    "shakespeare = pd.read_csv('data/shakespeare_plays.csv', delimiter=';', header=None, names=['line_id', 'play', 'something', 'something_else', 'character', 'text'])\n",
    "shakespeare.info()\n",
    "shakespeare.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b354cff7-5eb6-4515-bd56-eecba54b29d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Shakespeare play lines\n",
    "from nltk import word_tokenize\n",
    "from tqdm.auto import tqdm # for progress bar\n",
    "tqdm.pandas()\n",
    "\n",
    "def preprocess_shakespeare(text):\n",
    "    processed = text.lower()\n",
    "    return word_tokenize(processed)\n",
    "\n",
    "processed = shakespeare.text.dropna().progress_map(preprocess_shakespeare).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665b6f0e-0092-43a2-9840-e5329e41200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "n = # FILL IN what order of ngram\n",
    "shakespeare_input, vocab = padded_everygram_pipeline(n, processed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e35bdd-f7f8-49e1-9d8d-20d8a57b476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import Lidstone\n",
    "\n",
    "# Initiate and fit an ngram language model\n",
    "lm = Lidstone(gamma=0.001, order=n)\n",
    "lm.fit(shakespeare_input, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afebbdf8-2952-450e-aa56-ca26b01d3b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell as many times as you like to take new samples (generate new phrases)\n",
    "# Record or copy the cell if there are any good ones you want to save and report back to the clas\n",
    "\n",
    "num_tokens =  # FILL IN with how many tokens you want to generate\n",
    "prompt = [] # FILL IN with a list of tokens as a prompt (prior context). Or pass an empty list to just start generating\n",
    "generated_toks = lm.generate(num_words=num_tokens, text_seed=prompt)\n",
    "' '.join(prompt + generated_toks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
