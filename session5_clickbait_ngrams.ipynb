{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3dc1cc6-10e3-43e0-9b2a-9c3ab64ca08d",
   "metadata": {},
   "source": [
    "# N-gram document representations of headlines\n",
    "This time, we'll build and examine the ngram feature representations of documents in clickbait classification data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd1bb16-d13a-488c-800b-5f8fb4f58f27",
   "metadata": {},
   "source": [
    "## Setup\n",
    "No need to run this unless you haven't successfully installed `scikit-learn` and `nltk` from last class session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6443dda-9967-4c06-8dc9-ef35e1c3c2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install scikit-learn, a machine learning Python package\n",
    "! pip install --user scikit-learn\n",
    "! pip install --user nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5736dec-2e61-4429-85a6-4c1942cd55dc",
   "metadata": {},
   "source": [
    "Now select **Kernel > Restart Kernel** from the menu bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775107bc-b1f7-46b0-a5bc-43191c6715d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc00fe5-945f-4d1a-8089-8fbd3e945426",
   "metadata": {},
   "source": [
    "## Load clickbait data from Kaggle\n",
    "This data consists of headlines classified as clickbait or not (regular news). It is from a dataset on Kaggle, a site where machine learning competitions and datasets are often hosted. Source site: https://www.kaggle.com/datasets/amananandrai/clickbait-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90300de0-f52f-4122-a9bf-551271e1ff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the dataset with pandas\n",
    "# 0 corresponds to not clickbait, 1 has been judged as clickbait\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Set pandas to display entire texts in dataframes\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "data = pd.read_csv('data/clickbait_data.csv')\n",
    "data.info()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e8cc3e-5f01-448c-ad24-4522945d4f8a",
   "metadata": {},
   "source": [
    "## Split into training and test sets\n",
    "This isn't strictly necessary since we're not training a machine learning model with this notebook, but it is good practice to only \"train\" the vectorizer (figure out things like the vocabulary) from the training set. Otherwise you are \"looking\" at the test set and will get an overly optimistic estimate of performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0adbeff-1ebf-47b4-a88a-3711794e9681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_size = int(0.1 * len(data))\n",
    "train, test  = train_test_split(data, test_size=test_size, random_state=9)\n",
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d809d550-dfb8-4182-b288-ae65e56de1d6",
   "metadata": {},
   "source": [
    "## Extract **n-gram features** from the raw text data\n",
    "\"Features\" are data fields or attributes \"extracted\" from raw data, in our case, text data. The features were are examining here are \"unigram\" features, unique sequences of 1 word. This step converts each headline to a numeric vector of unigram counts (how many times each word type occurs).\n",
    "\"Training\" the vectorizer means finding how many unique features (in this case, unique words) are in the training set. This sets the number of columns in the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f76f8e-26ca-48b4-9a76-21f728f99a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "\n",
    "unigram_vectorizer = CountVectorizer(tokenizer=nltk.word_tokenize)\n",
    "unigram_vectorizer.fit(train['headline']) # input is a list of strings (documents)\n",
    "train_features = unigram_vectorizer.transform(train['headline'])\n",
    "test_features = unigram_vectorizer.transform(test['headline'])\n",
    "\n",
    "print(type(train_features))\n",
    "print(train_features.shape) # prints (number of rows in the matrix, number of columns)\n",
    "print(test_features.shape)  # prints (number of rows in the matrix, number of columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30f79a2-8b03-4557-90cc-b6a19e304fc8",
   "metadata": {},
   "source": [
    "Let's explore those training set unigram features a bit more. First convert the `scipy` sparse matrix into a regular `numpy` matrix to take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab18ef8-1ce6-47d8-a1ac-2c8b19b16e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_features = train_features.A\n",
    "print(type(unigram_features))\n",
    "print(unigram_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc38c76-77ef-4ee3-8e50-eb89176773b1",
   "metadata": {},
   "source": [
    "Each row in this matrix is a headline. Each column is the count of a unique word type. **How many words are there in the entire vocabulary of this dataset?**  \n",
    "Let's take a look at a few example headline vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171de09f-40b8-4408-92bf-7d9967fe45fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = 9 # FILL IN a random number less than the number of rows (datapoints) in ngram_features here\n",
    "train.iloc[sample_index] # Take a look at the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d596dd72-5b05-4107-bc34-f084dcb82d8c",
   "metadata": {},
   "source": [
    "How many values in this large, sparse vector aren't 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fc3e1b-1697-43a6-b02c-92b2491371f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.count_nonzero(unigram_features[sample_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ef672b-f8f6-4841-89d1-ce2389a28255",
   "metadata": {},
   "source": [
    "Label the nonzero features with the words they correspond to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2e833c-97ad-497a-9ecd-d9fd9a197d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a pandas dataframe from the ngram features and label the column with their corresponding feature (unigram or word type)\n",
    "feature_names = unigram_vectorizer.get_feature_names_out()\n",
    "print(len(feature_names))\n",
    "\n",
    "unigram_feature_matrix = pd.DataFrame(unigram_features, columns=feature_names)\n",
    "unigram_feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa1ef1c-b46f-4d04-ace7-3c51d1a4aa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the nonzero values in the feature vector for the example headline\n",
    "column_mask = unigram_feature_matrix.loc[sample_index].apply(lambda x: x > 0)\n",
    "nonzero_columns = column_mask[column_mask == True]\n",
    "unigram_feature_matrix.loc[[sample_index], nonzero_columns.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0011d359-3f82-465c-89e0-c49f00c2da85",
   "metadata": {},
   "source": [
    "## Extract bigram features\n",
    "Sequences of 2 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5767732-e02c-4af4-bd25-043ba0b8d1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(2,2), tokenizer=nltk.word_tokenize) # note the ngram_range parameter\n",
    "bigram_vectorizer.fit(train['headline'])\n",
    "train_bigram_features = bigram_vectorizer.transform(train['headline'])\n",
    "test_bigram_features = bigram_vectorizer.transform(test['headline'])\n",
    "\n",
    "print(train_bigram_features.shape) # prints (number of rows in the matrix, number of columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1708ba2d-ceb1-4c09-bfa8-68116e77f595",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_feature_matrix = pd.DataFrame(train_bigram_features.A, columns=bigram_vectorizer.get_feature_names_out())\n",
    "bigram_feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e302efcb-692b-4f55-bf2f-4cd332af6880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the nonzero values in the feature vector for the example headline\n",
    "column_mask = bigram_feature_matrix.loc[sample_index].apply(lambda x: x > 0)\n",
    "nonzero_columns = column_mask[column_mask == True]\n",
    "bigram_feature_matrix.loc[[sample_index], nonzero_columns.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dcfd2c-ee22-4fee-884f-81489eebbc4e",
   "metadata": {},
   "source": [
    "# Cosine similarity\n",
    "We can use the numeric feature vectors computed for every headline to compute similarities with other headlines using **cosine similarity**. Though contemporary information retreival (search engine) systems are of course much more complex, they still use this basic framework to return results: convert texts to vectors and return the most similar documents to your query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f58f9a-ea3a-4868-8809-166db32ebdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity between the sample headline vector and all other headlines in the training set\n",
    "\n",
    "from scipy.spatial.distance import cosine # cosine distance from the scipy package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d376b82-2990-4ea7-b05a-7325db3ca26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_vector = unigram_features[sample_index]\n",
    "sample_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77bf488-af1f-4d0d-b431-c83309a19d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity_to_sample(vector):\n",
    "    \"\"\" Compute cosine similarity with sample vector \"\"\"\n",
    "    return 1 - cosine(vector, sample_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314de55f-041e-46fa-836d-e4bb8279647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48849100-3486-47cd-bb85-ea4da7b74d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f35ef43-d5bf-492b-9f73-a35b863e3d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = unigram_feature_matrix.apply(compute_cosine_similarity_to_sample, axis=1) # apply function over every row in the df\n",
    "similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6ebc1-dfc2-4a8c-af28-954e4421eb19",
   "metadata": {},
   "source": [
    "A sanity check first. What should the sample vector's similarity with itself be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6a71c1-4d77-45b6-bf37-8b9323ea7c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities[sample_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54022db-5015-4473-9c76-12a581b24055",
   "metadata": {},
   "source": [
    "Now let's rank similarities and find out which vectors are most similar to the sample headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e722227-6017-4981-b4ef-fa31d3a28ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_similarities = similarities.sort_values(ascending=False)\n",
    "sorted_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc5538d-e3cb-46cf-b9bf-98cc3d4fedde",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.iloc[sorted_similarities.index[:10]] # Take a look at the top 10 most similar headlines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
